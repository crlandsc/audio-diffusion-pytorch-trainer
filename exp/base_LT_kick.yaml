# @package _global_

# Raw diffusion with Learned Transform (LT) pre-processing

# LT creates high freq artifacts

# To execute this experiment on a single GPU run:
# python train.py exp=base_kick trainer.gpus=1 datamodule.dataset.path=/your_wav_path_here

module: main.module_base
batch_size: 2
accumulate_grad_batches: 8 # use to increase batch size on single GPU effective batch size = (batch_size * accumulate_grad_batches)
num_workers: 8

sampling_rate: 44100
length: 131072 #65536 #32768
channels: 2
log_every_n_steps: 1000
# ckpt_every_n_steps: 4000 # Use if multiple checkpoints wanted

model:
  _target_: ${module}.Model
  lr: 1e-4
  lr_beta1: 0.95
  lr_beta2: 0.999
  lr_eps: 1e-6
  lr_weight_decay: 1e-3
  ema_beta: 0.995
  ema_power: 0.7

  model:
    _target_: main.DiffusionModel
    net_t:
      _target_: ${module}.UNetT_LT # The model type used for diffusion. Adjust LT params in module_base.py
      # num_filters: 32
      # window_length: 16
      # stride: 16
    in_channels: 2 # U-Net: number of input/output (audio) channels
    channels: [128, 128, 192, 192, 256, 256] # U-Net: channels at each layer
    factors: [1, 2, 2, 2, 2, 2] # U-Net: downsampling and upsampling factors at each layer
    items: [2, 2, 2, 2, 4, 4] # U-Net: number of repeating items at each layer
    attentions: [0, 0, 0, 0, 1, 1] # U-Net: attention enabled/disabled at each layer
    attention_heads: 8 # U-Net: number of attention heads per attention item
    attention_features: 64 # U-Net: number of attention features per attention item
    # Include specific diffusion or sampler types
    # diffusion_t:
    #   _target_: ${module}.DiffusionT # The diffusion method used (adjust in module_base.py)
    #   # net: ${model.model.net_t}
    # sampler_t:
    #   _target_: ${module}.SamplerT # The diffusion sampler used (adjust in module_base.py)
      # net: ${model.model.net_t}

    # # Include specific diffusion or sampler types
    # diffusion_t:
    #   _target_: main.VDiffusion # The diffusion method used
    #   net: ${model.model.net_t}
    # sampler_t:
    #   _target_: main.VSampler # The diffusion sampler used
    #   net: ${model.model.net_t}
    

# To specify train-valid datasets, datamodule must be reconfigured
datamodule:
  _target_: main.module_base.Datamodule
  dataset:
    _target_: audio_data_pytorch.WAVDataset
    path: ./data/wav_dataset/kicks # can overried when calling train.py
    recursive: True
    sample_rate: ${sampling_rate}
    transforms:
      _target_: audio_data_pytorch.AllTransform
      crop_size: ${length} # One-shots, so no random crop
      stereo: True
      source_rate: ${sampling_rate}
      target_rate: ${sampling_rate}
      loudness: -20
  val_split: 0.05
  batch_size: ${batch_size}
  num_workers: ${num_workers}
  pin_memory: True


callbacks:
  rich_progress_bar:
    # _target_: pytorch_lightning.callbacks.TQDMProgressBar
    _target_: pytorch_lightning.callbacks.RichProgressBar

  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "valid_loss"   # name of the logged metric which determines when model is improving
    save_top_k: 1           # save k best models (determined by above metric)
    save_last: True         # additionaly always save model from last epoch
    mode: "min"             # can be "max" or "min"
    verbose: False
    dirpath: ${logs_dir}/ckpts/${now:%Y-%m-%d-%H-%M-%S}
    filename: '{epoch:02d}-{valid_loss:.3f}'
    # every_n_train_steps: ${ckpt_every_n_steps} # Use if multiple checkpoints wanted

  model_summary:
    _target_: pytorch_lightning.callbacks.RichModelSummary
    max_depth: 2

  audio_samples_logger:
    _target_: main.module_base.SampleLogger
    num_items: 4
    channels: ${channels}
    sampling_rate: ${sampling_rate}
    length: ${length} # length of generated sample
    sampling_steps: [10, 50] # number of steps per sample
    use_ema_model: True
    # diffusion_sampler:
    #   _target_: main.VSampler
    #   net: ${model.model.net_t}
    # diffusion_schedule:
    #   _target_: main.LinearSchedule

loggers:
  wandb:
    _target_: pytorch_lightning.loggers.wandb.WandbLogger
    project: ${oc.env:WANDB_PROJECT}
    entity: ${oc.env:WANDB_ENTITY}
    name: unconditional_diffusion
    # offline: False  # set True to store all logs only locally
    job_type: "train"
    group: "" #"kick_1"
    save_dir: ${logs_dir}

trainer:
  _target_: pytorch_lightning.Trainer
  gpus: 1 # Set `1` to train on GPU, `0` to train on CPU only, and `-1` to train on all GPUs, default `0`
  precision: 16 # Precision used for tensors, default `32`, 16 for quicker runs on local machine
  accelerator: auto # default `None`
  min_epochs: 0
  max_epochs: -1
  enable_model_summary: False
  log_every_n_steps: 1 # Logs metrics every N batches
  # limit_val_batches: 10 # Use to limit the number of valid batches run (e.g. 10 stops training at 10 batches)
  check_val_every_n_epoch: null
  val_check_interval: ${log_every_n_steps}
  accumulate_grad_batches: ${accumulate_grad_batches} # use to increase batch size on single GPU